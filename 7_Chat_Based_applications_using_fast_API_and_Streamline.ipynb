{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNVpFInHC9Hw3vn6cF9luhm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itz-rakshitha5/Generative-AI/blob/main/7_Chat_Based_applications_using_fast_API_and_Streamline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iyh3prPY6aZ",
        "outputId": "24249d08-3a0c-4e88-e71b-312af4773531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0menv: CMAKE_ARGS=-DLLAMA_CUDA=on\n",
            "Collecting llama-cpp-python==0.2.69\n",
            "  Downloading llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.5/42.5 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (4.13.1)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.69)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.69-cp311-cp311-linux_x86_64.whl size=55723334 sha256=5e9318827a5703932b36a0b55cbf3b9a5987eb838c42e29abe075f8b1c962bab\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/1b/ff/b4dba97fbd16e731705b262602ba8f3b672bf4bde54ea0c104\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>0.1.6 \\\n",
        "            transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 \\\n",
        "            sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 \\\n",
        "            langchain_community\n",
        "\n",
        "%env CMAKE_ARGS=-DLLAMA_CUDA=on\n",
        "!pip install llama-cpp-python==0.2.69"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkQcfIXAjSH5",
        "outputId": "08c5a67c-34dd-4d41-d62d-2c2caee9fb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-09 08:06:15--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.40, 13.35.202.97, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.40|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1744189576&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDE4OTU3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=qNXUojpe5l7A5kapxclI5VAvSNVdQvvEY0H8y57E1aSElVufJEG5TxWYmc00L2mhuJhKWGvMjGYNpMGpqI2OYmJYlRlHoQjSYPHHM3SCZuy2uqmnjNTCiLzgPUwR1eXO4z8wx8646TnobjbR-KqJkz7nfVEHY4IurJvmbKNJwDe0V61XHg7iJ1VArudhWBK-Zb%7E7a0ZEnHxz80jilrvd0uz8EaGI1ovcfiH6Bj%7EfUYEIThBIfvM%7EAfSZACcZfJw6GpLctXf17YyBye2C4EjcG%7ENvzL%7EI6brRhYOWNE8rVYjNX9-s3-paBA0a5caDZpzh3-bEk82EY8-pZQjnxbWf3g__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-04-09 08:06:16--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1744189576&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDE4OTU3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=qNXUojpe5l7A5kapxclI5VAvSNVdQvvEY0H8y57E1aSElVufJEG5TxWYmc00L2mhuJhKWGvMjGYNpMGpqI2OYmJYlRlHoQjSYPHHM3SCZuy2uqmnjNTCiLzgPUwR1eXO4z8wx8646TnobjbR-KqJkz7nfVEHY4IurJvmbKNJwDe0V61XHg7iJ1VArudhWBK-Zb%7E7a0ZEnHxz80jilrvd0uz8EaGI1ovcfiH6Bj%7EfUYEIThBIfvM%7EAfSZACcZfJw6GpLctXf17YyBye2C4EjcG%7ENvzL%7EI6brRhYOWNE8rVYjNX9-s3-paBA0a5caDZpzh3-bEk82EY8-pZQjnxbWf3g__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.35.202.120, 13.35.202.113, 13.35.202.18, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.35.202.120|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
            "Saving to: ‘Phi-3-mini-4k-instruct-fp16.gguf’\n",
            "\n",
            "Phi-3-mini-4k-instr 100%[===================>]   7.12G   214MB/s    in 60s     \n",
            "\n",
            "2025-04-09 08:07:16 (121 MB/s) - ‘Phi-3-mini-4k-instruct-fp16.gguf’ saved [7643295904/7643295904]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"./Phi-3-mini-4k-instruct-fp16.gguf\",  # Adjust the path as needed\n",
        "    n_gpu_layers=-1,  # Offload all layers to GPU; adjust based on your GPU capacity\n",
        "    max_tokens=500,\n",
        "    n_ctx=2048,\n",
        "    seed=42,\n",
        "    verbose=False\n",
        ")"
      ],
      "metadata": {
        "id": "gvQUbUUhjUUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Alliance University got NAAC A+ Grade\""
      ],
      "metadata": {
        "id": "n93VYlHsjcQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = llm(\n",
        "    f\"<|user|>n{prompt}<|end|>\\n<|assistant|>\",\n",
        "    max_tokens=256,\n",
        "    stop=[\"<|end|>\"],\n",
        "    echo=True\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W10KtJ0QjghQ",
        "outputId": "e4890501-a32a-4aad-92c7-c249167d8847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-5d2add40765c>:1: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  output = llm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Congratulations to Alliance University for achieving the prestigious NAAC (National Assessment and Accreditation Council) A+ grade! This accolade is a testament to their commitment to educational excellence, innovation in teaching methodologies, quality of research outputs, infrastructure development, and overall management. It reflects positively on the institution's dedication to providing high-quality education and contributes significantly to its reputation among students, faculty, and global academic communities alike. Well done!\n"
          ]
        }
      ]
    }
  ]
}